---
title: 'Accelerate web site repairs: An R Markdown script for use with spidering and
  traffic data'
output:
  html_document:
    toc: yes
    toc_depth: 2
  html_notebook:
    toc: yes
    toc_depth: 2
---

dan.wendling@nih.gov; last updated: 7/6/2018.

See https://wiki.nlm.nih.gov/confluence/display/WEBDS/01+Acquire+data


In a nutshell:

(1) Add source files to /SEO_Spider/ and /traffic/ as described; run script; get results from /finished/.
(2) You may wish to update /IA_and_Governance_Struct/IA-Gov_Struct.csv and re-run. I usually run the script, open IA_and_Governance_Struct/updateTheseInMasterFile.csv, which is created by this script, to see how much needs to change, and weigh that against how much time I have this month. If I have time, I update IA-Gov_Struct.csv and kick off this script again to re-run.
(3) All opinions are my own.

Future, new calc: use this syntax, DateExpiration <  ymd(20170901))

==================================================================================


### If you don't know your default working directory...

It's hard to reliably import and export files if you haven't set a default working directory. This happens automatically if you have established an RStudio project that you copy the inbound files into, but you might still want to verify it. If needed, set a new working directory through the menu options, or with a command such as setwd ("Q:/_/_webDS/R"). Windows, setwd("C:/Documents/Text mining"); Mac, setwd ("/Users/wendlingd/Documents/webDS/R")

```{r}
getwd()
```



==================================================================================

## 1. Get started; create page table (data frame) from SEO Spider files

Select Ctrl-Alt-R to run the whole script, or there are variations (see the Run menu above).


### Load packages

If you have not added these packages to your R instance, turn this into a command and run:

install.packages("tidyverse", "stringr", "lubridate", "forcats", "ggvis", "jsonlite")



```{r}
library(tidyverse)
library(stringr)
library(lubridate)
library(forcats)
library(ggvis) # But you can comment this out and use ggplot2 if you want to, which will be loaded with tidyverse.
# library(jsonlite)
```

### Clear environment

If needed, clear all data frames from your work environment (Environment pane, see View > Panes in RStudio). Data frames require memory.

```{r}
rm(list=ls())
```



### Bring in the page data.

'skip' means to ignore row 1 - because SEO Spider puts the name of the file on row 1.

```{r}
internal_html <- read_csv("SEO_Spider/internal_html.csv", skip = 1)
```


### Wrangle the page data

View in new tab as you would view an Excel worksheet:

```{r}
# view(internal_html)
```

But it will probably be easier for you to click on the data frame names within the Environment pane. Use glimpse(item) as a commend if you want to see the data types column list for a data frame. Next:

1. Set column names to what they will be in MySQL.

2. Remove unneeded rows. The SEO Spider report includes URLs that failed to load during spidering (3xx and 4xx status codes). R explanation: http://r4ds.had.co.nz/transform.html#filter-rows-with-filter

3. Select only the variables (columns) we need. More: https://www.r-bloggers.com/the-complete-catalog-of-argument-variations-of-select-in-dplyr/

```{r}
pd1 <- internal_html %>% 
  rename(`Title` = `Title 1`,
        `MetaDescription` = `Meta Description 1`,
        `MetaKeyword` = `Meta Keyword 1`,
        `DateIssued` = `DateIssued 1`,
        `DateModified` = `DateModified 1`,
        `DateExpiration` = `DateExpiration 1`,
        `ContactEmail` = `ContactEmail 1`, 
        `DocumentType` = `DocumentType 1`,
        `RedirectUrl` = `Redirect URI`,
        `VAlign` = `valign 1`,
        `TableCount` = `TableCount 1`,
        `GtmStartInBody` = `GtmStartInBody 1`,
        `WebTrendsMentioned` = `WebTrendsMentioned 1`,
        `StatusCode` = `Status Code`) %>%
  filter(`StatusCode` == 200) %>%
  select(Address, Title, DateIssued, DateModified, DateExpiration, DocumentType, ContactEmail, RedirectUrl, MetaDescription, MetaKeyword, VAlign, TableCount, GtmStartInBody, WebTrendsMentioned, StatusCode) %>% 
  arrange(Address)
```



### Remove junk rows (If SEO Spider is reporting on non-www.nlm.nih.gov URLs)

After running a project to compare URLs ending in slash, the folder name, to URLs ending in a variation of index-dot-something (index.html, index.htm, index.asp, index.cfm, etc.), I determined that the URLs ending in slash can be removed. To re-run this project see IsItSafeToRemoveURLsEndingInSlash-Yes.txt. Future: See if SEO Spider can stop duplicating these entries. 

```{r}
pd2 <- pd1 %>% 
  filter(!grepl("/$", pd1$Address))
```

```{r}
pd2 <- pd2 %>% 
  filter(grepl("^https://www.nlm.nih.gov", pd2$Address))
```

Remove URLs containing a ? or a # - Future: try to eliminate in SEO Spider

```{r}
pd2 <- pd2 %>% 
  filter(!grepl("\\?", pd2$Address))
```

```{r}
pd2 <- pd2 %>% 
  filter(!grepl("\\#", pd2$Address))
```


### Build broken links data

#### Bring in

7/20/17: My machine has enough memory to process all_inlinks.csv, which allows me to consolidate several operations in this script, which I may switch to. Not all workstations have the memory to process all_inlinks.csv; for now I will take the easier route, although I have to bring in all_inlinks.csv for the PDF processing, later.


6/8/17: the 4xx file has 689 parsing failures. Can this be fixed?

Note: You have the option of going into the 'Status Code' variable/column and getting only 404's, or knocking out codes that you don't want to change. For now the below knocks out ~12 401 "unauthorized" links that are okay; these lead to a login form; for example https://www.nlm.nih.gov/exhibition/survivingandthriving/digitalgallery/detail-A025423.html. Knocking out 401 codes here seems safer than passing over them in SEO Spider or removing them from the reporting with a URL-based operation, I think. This way if it ever turned into a 404 it would appear in the report.

```{r}
LinkError4xx <- read_csv("SEO_Spider/client_error_(4xx)_inlinks.csv", skip = 1)
```


2018-07: Looks like SEO Spider labels our Support link, from many site areas, as broken. Here's how NLM
tech staff constructed the link: https://support.nlm.nih.gov/?from=https://www.nlm.nih.gov/bsd/disted/pubmedtutorial/cover.html
https://support.nlm.nih.gov/ics/support/ticketnewwizard.asp?style=classic&deptID=28054&from=

These should not register as broken; take them out.
Future: Cancel in SEO Spider if possible.

```{r}
LinkError4xx <- LinkError4xx %>% 
  filter(!grepl("support.nlm.nih", LinkError4xx$Destination))
```



```{r}
LinkError5xx <- read_csv("SEO_Spider/server_error_(5xx)_inlinks.csv", skip = 1)
```

Combine into one data frame

```{r}
brokenLinks <- bind_rows(LinkError4xx, LinkError5xx, id = NULL)
```


#### Remove junk observations (junk rows)

```{r}
ourBrokenLinks <- brokenLinks[grep("www.nlm.nih", brokenLinks$Source), ]
```

```{r}
ourBrokenLinks <- filter(ourBrokenLinks, Anchor != "Redirect")
```

401 means SEO Spider hit a login screen and SEO Spider has not been given the login credentials
```{r}
ourBrokenLinks <- filter(ourBrokenLinks, `Status Code` != "401")
```

```{r}
ourBrokenLinks <- filter(ourBrokenLinks, !grepl("/archive/", Source))
```




#### Update variable names (columns)

http://stackoverflow.com/questions/21752425/dplyr-mutate-in-r-add-column-as-concat-of-columns

```{r}
  cleanOurBrokenLinks <- ourBrokenLinks %>% 
  mutate(anchorDest = paste(Anchor, Destination, sep = ': ')) %>% 
  select(Source, anchorDest, Destination, Anchor, `Status Code`)
```


#### Derive brokenLinkCountByPage

```{r}
brokenLinkCountByPage <- cleanOurBrokenLinks %>% 
  count(Source, sort = TRUE) %>% 
  rename(bLinkCount = n)
```


#### Derive brokenLinkDataByPage

Verbose information about what the broken links are, aggregated up to the page level. 
http://stackoverflow.com/questions/28752805/use-dplyr-to-concatenate-a-column

```{r}
brokenLinkDataByPage <- cleanOurBrokenLinks %>% 
  group_by(Source) %>%
    summarise(bLinkData = paste(anchorDest, collapse = ". "))
```

```{r}
pd3 <- pd2 %>%
  left_join(brokenLinkCountByPage, c("Address" = "Source"))
```


#### Add variable brokenLinkDataByPage

```{r}
pd4 <- pd3 %>%
  left_join(brokenLinkDataByPage, c("Address" = "Source"))
```


If pd3 looks okay, that's all we need to retain in the environment. You can start over-writing things instead of creating new names, when you are sure no mistakes are getting in.

```{r}
rm(brokenLinkCountByPage)
rm(brokenLinkDataByPage)
rm(brokenLinks)
rm(cleanOurBrokenLinks)
rm(internal_html)
rm(LinkError4xx)
rm(LinkError5xx)
rm(ourBrokenLinks)
rm(pd1)
rm(pd2)
rm(pd3)
```
  


### Wrangle SWF data to create swfCountByPage

The file internal_flash.csv does NOT include the source URL (where the SWF was linked from), so for now I am getting SWF-related information from all_inlinks.csv.


#### Bring in

Huge file! 6/8/17: 112,400 parsing failures... How to resolve? but 8/4/17, 2.3M rows, imported okay (SEO Spider 8.1 recently installed)

```{r}
InLinks <- read_csv("SEO_Spider/all_inlinks.csv", skip = 1)
```


### Export table of http links for our http-to-https conversion

```{r}
httpLinks <- InLinks[grep("^http://", InLinks$Destination), ] %>% 
  arrange(Destination)
```

```{r}
httpLinks2 <- httpLinks[grep("nlm.nih.gov", httpLinks$Destination), ] %>% 
  arrange(Destination)
```

Huge file (~ 1/2 million lines) - takes a long time to write

```{r}
# write.csv(httpLinks2, file = "finished/httpLinks.csv", na="")
```


#### SWFs (Adobe Flash)

```{r}
SWFInLinks <- filter(InLinks, grepl("swf$", Destination))
```

We are targeting SWF videos for retirement, however the exhibitions area uses Flash technology that can't be migrated to our media player as the "targeted" SWF videos can be; so files residing in /exhibition/ are left out for now.

```{r}
SWFNotExhib <- filter(SWFInLinks, !grepl("gov/exhibition", Source))
```


#### swfCountByPage

```{r}
SWFCountTbl <- SWFNotExhib %>% 
  select(Source, Destination) %>% 
  count(Source, sort = TRUE) %>% 
  rename(swfCountByPage = n)
```

```{r}
pd5 <- pd4 %>%
  left_join(SWFCountTbl, c("Address" = "Source"))
```

If pd5 is okay, clear the memory

```{r}
rm(httpLinks)
#rm(httpLinks2)
rm(InLinks)
rm(pd4)
rm(SWFCountTbl)
rm(SWFInLinks)
rm(SWFNotExhib)
```


### Wrangle image data

```{r}
images <- read_csv("SEO_Spider/all_images.csv", skip = 1)
```


#### ImageCountPage

```{r}
ImageCountTbl <- images %>% 
  count(Source, sort = TRUE) %>% 
  rename(imageCountPage = n)
```

```{r}
pd6 <- pd5 %>%
  left_join(ImageCountTbl, c("Address" = "Source"))
```


#### totImageKiloPage

Create new column that converts Bytes to Kilobytes to reduce processing load and prevent "integer overflow" error

```{r}
ImgSizeToKilo <- images %>%
  rename(`Bytes` = `Size (Bytes)`) %>% 
  filter(!is.na(Bytes)) %>% 
  mutate(ImageKiloPage = round(Bytes / 1000)) 
```


#### Get summary image weight for the HTML page

```{r}
ImageSizeTbl <- ImgSizeToKilo %>% 
  group_by(Source) %>% 
  summarise(totImageKiloPage = sum(ImageKiloPage)) %>% 
  arrange(desc(totImageKiloPage)) %>%
  ungroup()
```

```{r}
pd7 <- pd6 %>%
  left_join(ImageSizeTbl, c("Address" = "Source"))
```


#### overweightImageCount - count of images per page that are over 200k

Count of images on each page that we consider to be too big for smartphones. SEO Spider has a report of images over 100k; this might be too low for our site type; setting to a 200k (200000 bytes) threshold for now...

```{r}
ImgOverweightByPage <- ImgSizeToKilo %>%
  filter(!is.na(Bytes), Bytes > 200000 ) %>% 
  count(Source, sort = TRUE) %>% 
  rename(overweightImageCount = n)
```

```{r}
pd8 <- pd7 %>%
  left_join(ImgOverweightByPage, c("Address" = "Source"))
```


#### Images lacking alt text, noAltCountPg, noAltData

```{r}
noAltText <- read_csv("SEO_Spider/images_missing_alt_text_inlinks.csv", skip = 1)
```

8/9/2017: 13k rows list the Source _folder_ rather than the source URL. I assume that SOME of these are dupes. Future: Project to determine whether to count these, or not. Removing for now.

```{r}
noAltText2 <- noAltText %>% 
  filter(!grepl("/$", noAltText$Source)) %>% 
  arrange(Source)
```


```{r}
noAltTextcountByPage <- noAltText2 %>% 
  count(Source, sort = TRUE) %>% 
  rename(noAltCountPg = n)
```

```{r}
noAltDataByPage <- noAltText2 %>% 
  group_by(Source) %>%
    summarise(noAltData = paste(Destination, collapse = ". "))
```

```{r}
pd9 <- pd8 %>%
  left_join(noAltTextcountByPage, c("Address" = "Source"))
```

```{r}
pd10 <- pd9 %>%
  left_join(noAltDataByPage, c("Address" = "Source"))
```


If pd10 looks okay,

```{r}
rm(ImageCountTbl)
rm(images)
rm(ImageSizeTbl)
rm(ImgOverweightByPage)
rm(ImgSizeToKilo)
rm(pd5)
rm(pd6)
rm(pd7)
rm(pd8)
rm(pd9)
rm(noAltDataByPage)
rm(noAltText)
rm(noAltText2)
rm(noAltTextcountByPage)
```


==================================================================================


## 2. Apply site architecture and governance structure / organizational ownership

If you put this data directly into your web CMS, you can scrape it from pages using SEO Spider's Custom Extract feature, and skip this section!

Now, assign to each row of the table (32,000+ pages in this case):

 1. The name of the org chart piece that owns this information
 2. The name of the microsite / communication package that the page belongs to (around 400 in this case)
 
This is what allows R to build a top-level summary for the web site. Easiest to build in a database, but Excel would also work. In my version, one row in this file will correspond to one row in internal_html.csv. I maintain this in a database, where it's easiest to update. If you don't have a database, Excel will be fine (but with extra work), and if you remember to save as CSV. My arrangement, row 1 and a sample row:

Address,ContentSteward,StewardTopCat,StewardCode,ContentGroup,templateName
"https://www.mysite.gov/index.html","Public Affairs","OD-PA",4,"Research Fellowship Program","MainTemplate"

*Address*: The URL of the page you are cataloging. Meant to be joinable with any other data regarding repair/diagnostic/traffic/etc.

*ContentSteward*: A human-readable but short hierarchical path, starting from the top of the organization and listing all organizational units "down" to the most specific team name that has permission to remove the content / is responsible for the content. The name 'Product Manager' may also apply here. In the example, OD-PA, means Office of the Director > Public Affairs. This tag is used in database queries and in JavaScript controls about ownership/governance, and in some chart labels.

*StewardTopCat*: Dictates where in the overall site summary chart this data will appear. The top category (org chart entry) that maintains the page in the web CMS. Content "owned" by an organizational division is not edited by the office of the director, so the division should be listed here. Whereas ContentSteward locates the lowest team responsible, the StewardTopCat locates the highest group responsible. 

*StewardCode*: A numeric representation of StewardTopCat for use by the database / JavaScript.

*ContentGroup*: Name of the thematic group to which this URL belongs; the product or service that a group of pages is about. Other names could be the communication package, the microsite name, the content group, etc.

*templateName*: Some of our web work is to make pages responsive based on their web CMS template, change to an archive/retirement template, etc. Knowing the web CMS designation for the page type can be helpful.

```{r}
IAGovStruct <- read_csv("IA_and_Governance_Struct/IA-Gov_Struct.csv")
```

FYI, how the columns are indexed

```{r}
glimpse(IAGovStruct)
```




### Add IA/Gov in

```{r}
pd11 <- pd10 %>% 
  left_join(IAGovStruct, c("Address" = "Address"))
```



### Check for entries with no assignment, repair and re-run as needed

Look at pages that lack the new assignments. If necessary, update the csv file and re-run this section.

```{r}
pd11NoAssignment <- pd11 %>% 
  filter(is.na(StewardTopCat)) %>% 
  arrange(Address)
```

View the above in RStudio; best case: there are zero rows shown. Any entries here will NOT be tagged for IA and governance unless you update the mapping file and run this whole procedure again.

```{r}
write.csv(pd11NoAssignment, file = "IA_and_Governance_Struct/updateTheseInMasterFile.csv", na="")
```

If pd11 looks okay (check pd11NoAssignment!)

First round of mods complete for the data frame describing our pages. Change name to "pageData"

```{r}
pageData <- pd11
```

```{r}
rm(IAGovStruct)
rm(pd10)
rm(pd11)
rm(pd11NoAssignment)
```


==================================================================================


## 3. Bring in traffic data from Google Analytics, /traffic/

A connection to Google Analytics can be set up in SEO Spider, making this step unnecessary; see https://www.screamingfrog.co.uk/seo-spider/user-guide/configuration/#google-analytics-integration. For now I'm using GA's standard feature of exporting an unsampled report from GA, and then importing it here.

Future objective: Derive a 12-month average number of Unique Page Views per month. Because some of our content is seasonal, I think this will elevate important content to attention, that might otherwise be missed.

Use the 'skip = 6' argument when you told GA at export that you wanted the strategy included in the file.

8/4/17: Testing with 10 months of data. Will switch to 12 when our account goes back that far.

```{r}
trafficNew <- read_csv("traffic/Pages.csv", skip = 6)
```


### Check to see if GAP reports folder names rather than the complete address

SEO Spider data includes folderName/fileName. These won't be join-able if there are GAP entries with merely folderName/.


### Standardize variable (column) names, add aveMonthlyUniquePV

Below assumes a 12-month export from Google Analytics! Change the 'divided by' as needed.

```{r}
cleanTraff1 <- trafficNew %>% 
  rename(`Address` = `Page`,
         `TotalPageViews` = `Pageviews`,
         `UniquePageViews` = `Unique Pageviews`,
         `AveTimeOnPage` = `Avg. Time on Page`,
         `BounceRate` = `Bounce Rate`,
         `PercentExit` = `% Exit`,
         `PageValue` = `Page Value`) %>%
  mutate(aveMonthlyUniquePV = round(UniquePageViews / 12)) %>% 
  select(Address, aveMonthlyUniquePV, UniquePageViews, TotalPageViews, Entrances, BounceRate, PercentExit)
```


### Require Address to start with www.nlm.nih.gov/

Lots of entries for Google Translate, etc., that are interesting but I'm not studying translation now. Also lots of non-NLM entries - not sure where those are coming from.

```{r}
cleanTraff2 <- cleanTraff1[grep("^www.nlm.nih.gov/", cleanTraff1$Address), ]
```


### Example of removing unneeded entries

Eventually I would like to use GAP to get accurate counts, and currently there is a lot of junk in the file, including malformed URLs and deleted/moved pages (if we do long timespans this could become a processing issue).

```{r}
cleanTraff3 <- filter(cleanTraff2, !grepl("www.nlm.nih.gov/medlinepl", Address))
```

### Create report of folder names in GAP

```{r}
# GapUrlEndsWithFolderName <- cleanTraff3 %>% 
#  filter(grepl("/$", cleanTraff3$Address))
```

```{r}
#write.csv(GapUrlEndsWithFolderName, file = "finished/GapUrlEndsWithFolderName.csv", na="")
```



### Make traffic Address join-able

#### Add https:// to start of Address

https://blog.exploratory.io/7-most-practically-useful-operations-when-wrangling-with-text-data-in-r-7654bd9d1a0c. Uses library(stringr), loaded at top.

```{r}
cleanTraff4 <- cleanTraff3 %>% 
  mutate(`Address` = str_replace(`Address`, "^www", "https://www"))
```

#### Add index.html to end of Address when the GA URL ends with the directory, i.e. / 

```{r}
cleanTraff5 <- cleanTraff4 %>% 
  mutate(`Address` = str_replace(`Address`, "/$", "/index.html"))
```


#### Add traffic to the pageData data frame

http://r4ds.had.co.nz/relational-data.html#mutating-joins

```{r}
pageData <- pageData %>% 
  left_join(cleanTraff5, by = "Address")
```


#### Add page rank

Note: Pages with the same traffic count will have the same rank count. This is intentional; it's a "tie" when pages have the same traffic.

http://r4ds.had.co.nz/transform.html#grouped-summaries-with-summarise

```{r}
pageData <- pageData %>% 
  mutate(trafficRankPage = min_rank(desc(UniquePageViews)))
```


If pageData is okay, you can free up memory

```{r}
rm(cleanTraff1)
rm(cleanTraff2)
rm(cleanTraff3)
rm(cleanTraff4)
rm(cleanTraff5)
rm(trafficNew)
```


test <- pageData %>% 
  filter(grepl("cdm", pageData$ContentGroup))

pd2 <- pd2 %>% 
  filter(grepl("^https://www.nlm.nih.gov", pd2$Address))



```
HMDmain <- pageData %>% 
  filter(grepl("^OD-LO-HMD-Main", pageData$ContentSteward))
```



==================================================================================


## 4. Create groupData - a summary data frame


### New data frame: groupData with basics

ContentGroup, ContentSteward, pageCountGr (Total static HTML pages published)

```{r}
gd1 <- pageData %>% 
  group_by(ContentGroup, ContentSteward, StewardTopCat, StewardCode) %>% 
  summarise(n = n()) %>% 
  rename(`pageCountGr` = n) %>% 
  select(ContentGroup, ContentSteward, StewardCode, StewardTopCat, pageCountGr)
```

FUTURE: Need to find a way to (1) view unmapped pages; (2) update mappings to collect unmatched rows; (3) integrate the new ones, or re-rerun from the start.


### docTypesUsed

Results in, "Adding missing grouping variables." Seems to run correctly, though.

```{r}
docTypesUsed <- pageData %>% 
  select(ContentGroup, ContentSteward, DocumentType) %>%   
  group_by(ContentSteward, ContentGroup) %>%
  distinct(DocumentType) %>% select(DocumentType) %>% 
  summarise(docTypesUsed = paste(DocumentType, collapse = " | "))
```

```{r}
gd2 <- gd1 %>%
  left_join(docTypesUsed, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### emailsUsed

```{r}
emailsUsed <- pageData %>% 
  select(ContentGroup, ContentSteward, ContactEmail) %>%
  group_by(ContentSteward, ContentGroup) %>%
  distinct(ContactEmail) %>% select(ContactEmail) %>% 
  summarise(emailsUsed = paste(ContactEmail, collapse = " | "))
```

```{r}
gd3 <- gd2 %>%
  left_join(emailsUsed, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### aveMonthlyUniquePVsGr and trafficRankGr

```{r}
UniquePVsGr <- pageData %>%
  filter(!is.na(UniquePageViews)) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(aveMonthlyUniquePVsGr = sum(UniquePageViews)) %>% 
  arrange(desc(aveMonthlyUniquePVsGr)) %>%
  ungroup() %>% 
  mutate(trafficRankGr = min_rank(desc(aveMonthlyUniquePVsGr)))
```

```{r}
gd4 <- gd3 %>%
  left_join(UniquePVsGr, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### bLinkCntGr

```{r}
brokenLinkTotsGr <- pageData %>%
  filter(!is.na(bLinkCount)) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(bLinkCntGr = sum(bLinkCount)) %>% 
  arrange(desc(bLinkCntGr)) %>%
  ungroup()
```

```{r}
gd5 <- gd4 %>%
  left_join(brokenLinkTotsGr, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### swfCountGr

```{r}
SWFCountTots <- pageData %>%
  filter(!is.na(swfCountByPage)) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(swfCountGr = sum(swfCountByPage)) %>% 
  arrange(desc(swfCountGr)) %>%
  ungroup()
```

```{r}
gd6 <- gd5 %>%
  left_join(SWFCountTots, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### imageCountGr

```{r}
imageTots <- pageData %>%
  filter(!is.na(imageCountPage)) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(imageCountGr = sum(imageCountPage)) %>% 
  arrange(desc(imageCountGr)) %>%
  ungroup()
```

```{r}
gd7 <- gd6 %>%
  left_join(imageTots, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### imageWeightMBGr

```{r}
imageWeightTots <- pageData %>%
  filter(!is.na(totImageKiloPage)) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(imageWeightMBGr = sum(totImageKiloPage) / 1000) %>% 
  arrange(desc(imageWeightMBGr)) %>%
  ungroup()
```

```{r}
gd8 <- gd7 %>%
  left_join(imageWeightTots, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### pageCountWithOverweightImg from overweightImageCount
For now we're defining overweight as more than 200k (200,000 bytes, see overweightImageCount above). SEO Spider has a report for images over 100k.

```{r}
imageTotsOverweight <- pageData %>%
  filter(!is.na(overweightImageCount)) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(pageCountWithOverweightImg = sum(overweightImageCount)) %>% 
  arrange(desc(pageCountWithOverweightImg)) %>%
  ungroup()
```

```{r}
gd9 <- gd8 %>%
  left_join(imageTotsOverweight, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### expiredPagesCntGr (Expired pages)

```{r}
nowExpired <- pageData %>% 
  filter(!is.na(DateExpiration), !is.na(ContentSteward), DateExpiration <  ymd(20170901)) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(expiredPagesCntGr = n()) %>% 
  arrange(desc(expiredPagesCntGr)) %>%
  ungroup()
```

```{r}
gd10 <- gd9 %>%
  left_join(nowExpired, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


If gd10 looks okay, remove,

```{r}
rm(brokenLinkTotsGr)
rm(docTypesUsed)
rm(emailsUsed)
rm(gd1)
rm(gd2)
rm(gd3)
rm(gd4)
rm(gd5)
rm(gd6)
rm(gd7)
rm(gd8)
rm(gd9)
rm(imageTots)
rm(imageTotsOverweight)
rm(imageWeightTots)
rm(nowExpired)
rm(SWFCountTots)
rm(UniquePVsGr)
```


### recentEditsCntGr (Pages edited recently - Last 3 months, from DateModified)

Date calc for this and next uses today's date and the lubridate package.  https://blog.exploratory.io/filter-with-date-function-ce8e84be680. You could also hard-code a date to go back to, such as "DateModified >  ymd(20170308)".

```{r}
recentEdits <- pageData %>% 
  filter(!is.na(DateModified), !is.na(ContentSteward), DateModified >= today() - days(90)) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(recentEditsCntGr = n()) %>% 
  arrange(desc(recentEditsCntGr)) %>%
  ungroup()
```

```{r}
gd11 <- gd10 %>%
  left_join(recentEdits, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### recentAddsCntGr

```{r}
recentAdds <- pageData %>% 
  filter(!is.na(DateIssued), !is.na(ContentSteward), DateIssued >= today() - days(90)) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(recentAddsCntGr = n()) %>% 
  arrange(desc(recentAddsCntGr)) %>%
  ungroup()
```

```{r}
gd12 <- gd11 %>%
  left_join(recentAdds, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### pgsWithVAlignCntGr

```{r}
VAlignTots <- pageData %>%
  filter(!is.na(VAlign)) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(pgsWithVAlignCntGr = n()) %>% 
  arrange(desc(pgsWithVAlignCntGr)) %>%
  ungroup()
```

Could be:
  count(Source, sort = TRUE) %>% 
  rename(swfCountByPage = n)
  
```{r}
gd13 <- gd12 %>%
  left_join(VAlignTots, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### TableCountTotGr

```{r}
TableCountTots <- pageData %>%
  filter(!is.na(TableCount), TableCount > 0 ) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(TableCountTotGr = n()) %>% 
  arrange(desc(TableCountTotGr)) %>%
  ungroup()
```

```{r}
gd14 <- gd13 %>%
  left_join(TableCountTots, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```

  count(Source, sort = TRUE) %>% 
  rename(swfCountByPage = n)
  

### GtmStartInBodyCntGr

```{r}
GtmStartTots <- pageData %>%
  filter(!is.na(GtmStartInBody), GtmStartInBody > 0 ) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(GtmStartInBodyCntGr = n()) %>% 
  arrange(desc(GtmStartInBodyCntGr)) %>%
  ungroup()
```

```{r}
gd15 <- gd14 %>%
  left_join(GtmStartTots, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### WebTrendsMentionedCntGr

```{r}
WebTrendsTots <- pageData %>%
  filter(!is.na(WebTrendsMentioned), WebTrendsMentioned > 0 ) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(WebTrendsMentionedCntGr = n()) %>% 
  arrange(desc(WebTrendsMentionedCntGr)) %>%
  ungroup()
```

```{r}
gd16 <- gd15 %>%
  left_join(WebTrendsTots, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### pgsLackingEmailCtnGr

```{r}
noEmailTots <- pageData %>%
  filter(is.na(ContactEmail)) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  summarise(pgsLackingEmailCtnGr = n()) %>% 
  arrange(desc(pgsLackingEmailCtnGr)) %>%
  ungroup()
```

```{r}
gd17 <- gd16 %>%
  left_join(noEmailTots, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


### earliestDateIssuedGr

```{r}
earliestIssuedPage <- pageData %>%
  filter(!is.na(DateIssued)) %>% 
  group_by(ContentGroup, ContentSteward) %>% 
  arrange(DateIssued) %>%
  slice(1L) %>% 
  rename(`earliestDateIssuedGr` = `DateIssued`) %>% 
  select(ContentGroup, ContentSteward, earliestDateIssuedGr)
```

```{r}
gd18 <- gd17 %>%
  left_join(earliestIssuedPage, by = c("ContentGroup" = "ContentGroup", "ContentSteward" = "ContentSteward"))
```


old - repairTotCnt

Future: Add a total repair count? But SQL can do this: UPDATE content_group SET repairTotCount = brokenLinksGr + recordingCountSWFGr + nonHtmlCountGr;

If gd18 is okay, 

```{r}
groupData <- gd18
```


```{r}
rm(earliestIssuedPage)
rm(gd10)
rm(gd11)
rm(gd12)
rm(gd13)
rm(gd14)
rm(gd15)
rm(gd16)
rm(gd17)
rm(gd18)
rm(recentAdds)
rm(recentEdits)
```


### Put the columns/variables into a logical reading order

UPDATE THIS EVERY TIME YOU ADD A NEW COLUMN. Use this when you want to create a more-logical column order for people using the data in R, Excel, etc. If you add more columns in the future, you can run glimpse to review the column order.

```{r}
pageData = pageData %>% 
  select(StewardCode, StewardTopCat, Address, Title, ContentSteward, ContentGroup, ContactEmail, aveMonthlyUniquePV, trafficRankPage, DateIssued, DateModified, DateExpiration, bLinkCount, bLinkData, swfCountByPage, imageCountPage, totImageKiloPage, noAltCountPg, noAltData, overweightImageCount, DocumentType, RedirectUrl, MetaDescription, MetaKeyword, VAlign, TableCount, GtmStartInBody, WebTrendsMentioned, UniquePageViews, TotalPageViews, Entrances, BounceRate, PercentExit)
```

```{r}
groupData = groupData %>% 
  select(ContentGroup, pageCountGr, StewardCode, StewardTopCat, ContentSteward, aveMonthlyUniquePVsGr, trafficRankGr, bLinkCntGr, swfCountGr, imageCountGr, imageWeightMBGr, pageCountWithOverweightImg, expiredPagesCntGr, recentEditsCntGr, recentAddsCntGr, docTypesUsed, emailsUsed, pgsWithVAlignCntGr, TableCountTotGr, GtmStartInBodyCntGr, WebTrendsMentionedCntGr, pgsLackingEmailCtnGr, earliestDateIssuedGr)
```



========================================================================================

## 5. A few visualizations

#### Note about the date calculation used to create some charts

Several charts describe recent activities, using date calculations such as "today() - days(90)." These are most accurate right after a new data set has been created; if you're running these at the beginning of each reporting cycle, this will work. If not, you might consider different coding.


### Page count by content steward

```{r}
chOwnerPageCount <- groupData %>% 
  group_by(StewardTopCat) %>% 
  summarise(totPagesOwner = sum(pageCountGr)) %>% 
  arrange(desc(totPagesOwner)) %>%
  select(StewardTopCat, totPagesOwner)
```

```{r}
ggplot(chOwnerPageCount, aes(x = reorder(StewardTopCat, totPagesOwner), y = totPagesOwner)) + geom_bar(stat = "identity") + coord_flip() + labs(title = "Page count by content steward") + geom_text(aes(label = totPagesOwner), position = position_dodge(0.9), hjust="inward")
```


Throughout this section, viewing ggvis charts is optional. If you chose to install ggvis:

```{r}
  chOwnerPageCount %>% ggvis(x = ~StewardTopCat, y = ~totPagesOwner) %>% layer_bars()
```


### Traffic by owner (Average monthly unique page views)

```{r}
chOwnerAveMoPV <- groupData %>% 
  filter(!is.na(aveMonthlyUniquePVsGr), !is.na(ContentSteward)) %>% 
  group_by(StewardTopCat) %>% 
  summarise(totAveMonthlyUniquePgViews = sum(aveMonthlyUniquePVsGr)) %>% 
  arrange(desc(totAveMonthlyUniquePgViews)) %>%
  select(StewardTopCat, totAveMonthlyUniquePgViews)
```

```{r}
ggplot(chOwnerAveMoPV, aes(x = reorder(StewardTopCat, totAveMonthlyUniquePgViews), y = totAveMonthlyUniquePgViews)) + geom_bar(stat = "identity") + coord_flip() + labs(title = "Traffic by owner", subtitle = "(Average monthly unique page views)") + geom_text(aes(label = totAveMonthlyUniquePgViews), position = position_dodge(0.9), hjust="inward")
```


### Top 20 communication packages by traffic (Average monthly unique page views)

```{r}
chGroupAveMoPV <- groupData %>% 
  filter(!is.na(aveMonthlyUniquePVsGr), !is.na(ContentGroup)) %>% 
  group_by(ContentGroup) %>% 
  summarise(totAveMonthlyUniquePgViewsGr = sum(aveMonthlyUniquePVsGr)) %>% 
  arrange(desc(totAveMonthlyUniquePgViewsGr)) %>%
  select(ContentGroup, totAveMonthlyUniquePgViewsGr) %>% 
  slice(1:20)
```

```{r, fig.width=10,fig.height=7}
ggplot(chGroupAveMoPV, aes(x = reorder(ContentGroup, totAveMonthlyUniquePgViewsGr), y = totAveMonthlyUniquePgViewsGr)) + geom_bar(stat = "identity") + coord_flip() + labs(title = "Top 20 communication packages by traffic", subtitle = "(Average monthly unique page views)") + geom_text(aes(label = totAveMonthlyUniquePgViewsGr), position = position_dodge(0.9), hjust="inward")
```



### Top editing activity by specific owner, email owner (templated pages, past 3 months)

Change this to StewardTopCat (add this to the df)

```{r}
chEditsBySteward <- pageData %>% 
  filter(!is.na(DateModified), !is.na(ContentSteward), DateModified >= today() - days(90), DateModified <= today()) %>% # Outliers can appear when an automated process updates many pages at once, something I don't need to measure. These can be removed by adding to filter - ContentSteward != "OD-PA"
  group_by(ContentSteward) %>% 
  summarise(Edits = n()) %>%
  arrange(desc(Edits)) %>%
  select(ContentSteward, Edits)
```

```{r}
ggplot(chEditsBySteward, aes(x = reorder(ContentSteward, Edits), y = Edits)) + geom_bar(stat = "identity") + coord_flip() + labs(title = "Top editing activity by specific owner", subtitle = "(templated pages, past 3 months)") + geom_text(aes(label = Edits), position = position_dodge(0.9), hjust="inward")
```


For ggvis, similar to above, but time-series line chart where you can filter by org owner.

```{r}
chEditsByStewardViz <- pageData %>% 
  filter(!is.na(DateModified), !is.na(ContentSteward), DateModified >= today() - days(90), DateModified <= today()) %>%
  group_by(DateModified, ContentSteward) %>% 
  summarise(Edits = n()) %>%
  arrange(DateModified) %>%
  select(DateModified, ContentSteward, Edits)
```

View all first with "filter" commented out. Then uncomment the filter and add EXACT name...  Separators: use a comma when you want to use "and"; use a pipe | character when you want to use "or"

```{r}
chEditsByStewardViz %>% 
  filter(ContentSteward == "OD-LO-BSD-MMS")  %>% # | ContentSteward == "OD-LO-PSD-RWS-WIM")
  ggvis(~DateModified, ~Edits, stroke = ~ContentSteward) %>% layer_points() %>%  layer_lines()
```

Edits by email address:

```{r}
chEditsByEmailViz <- pageData %>% 
  filter(!is.na(DateModified), !is.na(ContactEmail), DateModified >= today() - days(90), DateModified <= today()) %>%
  group_by(DateModified, ContactEmail) %>% 
  summarise(Edits = n()) %>%
  arrange(DateModified) %>%
  select(DateModified, ContactEmail, Edits)
```


chEditsByEmailViz %>% 
  filter(ContentSteward == "OD-LO-PSD-RWS-USU" | ContentSteward == "OD-LO-PSD-RWS-WIM") %>%
  filter(ContactEmail != "wwwteam@mysite.gov") %>% 
  ggvis(~DateModified, ~Edits, stroke = ~ContactEmail) %>% layer_points() %>%  layer_lines()




### Top editing activity by communication package (templated pages, past 3 months)

R will add the grouping variables in to the output; apparently it has to if I do all these in one step.

```{r}
chEditsByContentGr <- groupData %>% 
  filter(!is.na(recentEditsCntGr), recentEditsCntGr > 5, !is.na(ContentSteward)) %>% 
  # Place to remove outliers, see above
  arrange(desc(recentEditsCntGr)) %>% 
  select(ContentGroup, recentEditsCntGr)
```

## MMS

```{r}
chEditsByContentGrMMS <- pageData %>% 
  filter(!is.na(DateModified), !is.na(ContentSteward), ContentSteward == "OD-LO-BSD-MMS") %>% 
  arrange(desc(DateModified)) %>% 
  select(ContentGroup, DateModified)
```


```{r}
#  filter(DateModified > "2018-04-01" & DateModified < "2018-07-01")
```

```{r}
write.csv(chEditsByContentGrMMS, file = "finished/MMS-3mo.csv", na="")
```


```
{r, fig.width=10,fig.height=11}
ggplot(chEditsByContentGr, aes(x = reorder(ContentGroup, recentEditsCntGr), y = recentEditsCntGr)) + geom_bar(stat = "identity") + coord_flip() + labs(title = "Top editing activity by communication package", subtitle = "(templated pages, past 3 months)") + geom_text(aes(label = recentEditsCntGr), position = position_dodge(0.9), hjust="inward")
```


For ggvis, similar to above, but time-series line chart where you can filter for the communication packages of one org owner.

```{r}
chEditsByCommPkgViz <- pageData %>% 
  filter(!is.na(DateModified), !is.na(ContentGroup), DateModified >= today() - days(90), DateModified <= today(), ContentSteward == "OD-LO-BSD-MMS") %>%
  group_by(DateModified, ContentSteward, ContentGroup) %>% 
  summarise(Edits = n()) %>%
  arrange(DateModified) %>%
  select(DateModified, ContentSteward, ContentGroup, Edits)
```

Filter requires EXACT names, which can be found above. Separators: For "and" use comma; for "or" use pipe |

```{r}
chEditsByCommPkgViz %>% 
#  filter(ContentSteward == "OD-LO-PSD-RWS-USU" | ContentSteward == "OD-LO-PSD-RWS-WIM") %>%
  ggvis(~DateModified, ~Edits, fill = ~ContentGroup) %>% layer_points()
```



### Editing activity by date (templated pages, past 6 months)

Needed to restrict DateModified to dates today or before, because of outliers saying they were updated 2018-07. Also UMLS outlier.

```{r}
chEditsByDate <- pageData %>% 
  filter(!is.na(DateModified), !is.na(ContentSteward), DateModified >= today() - days(180), DateModified <= today(), ContentGroup != "UMLS-Other") %>% 
  group_by(DateModified) %>% 
  summarise(Edits = n()) %>%
  select(DateModified, Edits)
```

```{r, fig.width=10,fig.height=4}
ggplot(chEditsByDate, aes(x = DateModified, y = Edits)) + geom_bar(stat = "identity") + labs(title = "Editing activity by date", subtitle = "(templated pages, past 6 months)")
```


### Broken links by communication package, counts for the "top 20"

slice(1:20 not working, not sure why. Filter is base R, but arrange not working? https://stackoverflow.com/questions/25994307/filtering-data-frame-based-on-row-number.

Broke this, please fix:

 chbLinksByContentGr <- groupData %>% 
   filter(!is.na(bLinkCntGr), !is.na(ContentGroup)) %>%
   summarise(bLinks = n()) %>%
   arrange(desc(bLinkCntGr)) %>% 
   select(ContentGroup, bLinkCntGr) %>% 
   filter(row_number() <= 20, row_number() >= 2)

 ggplot(chbLinksByContentGr, aes(x = reorder(ContentGroup, bLinkCntGr), y = bLinkCntGr)) + geom_bar(stat = "identity") + coord_flip() + labs(title = "Communication packages with broken links", subtitle = "Top 20 by broken link count")



### Broken links by email owner

Still working on this.

chEditsByEmailViz <- pageData %>% 
  filter(!is.na(DateModified), !is.na(ContactEmail), DateModified >= today() - days(90), DateModified <= today()) %>%
  group_by(DateModified, ContactEmail) %>% 
  summarise(Edits = n()) %>%
  arrange(DateModified) %>%
  select(DateModified, ContactEmail, Edits)
  

chbLinksByContentGr <- pageData %>% 
  filter(!is.na(bLinkCount), !is.na(ContentGroup)) %>%
  group_by(ContactEmail) %>% 
  summarise(bLinks = n()) %>%
  arrange(desc(bLinks)) %>% 
  select(ContactEmail, bLinks) %>% 
  filter(row_number() <= 20, row_number() >= 2)
  
  

```{r}
chbLinksByEmail <- pageData %>% 
  filter(!is.na(DateModified), !is.na(ContactEmail), DateModified >= today() - days(90), DateModified <= today()) %>%
  group_by(ContactEmail) %>% 
  summarise(bLinks = n()) %>%
  arrange(desc(bLinks)) %>% 
  select(ContactEmail, bLinks)
```

```{r}
ggplot(chbLinksByEmail, aes(x = reorder(ContactEmail, bLinks), y = bLinks)) + geom_bar(stat = "identity") + coord_flip() + labs(title = "Broken links by email owner") + geom_text(aes(label = bLinks), position = position_dodge(0.9), hjust="inward")
```




### Report on the 'broken links repair' initiative

Still working

What proportion of the broken links reported 30 days ago have been fixed?

Stacked bar, numbers not percent, by owner, to show volume by owner and the recent activity to fix them (or the lack of recent activity to fix them). Future: third element - height of top bar is the total count of links owned (excluding topnav, bottomnav, etc.)

Last month's broken links report
This month's broken links report

Could be link count reported, link count resolved; or, 

The number of pages that were edited to remove the broken links?

rm(chbLinksByContentGr)
rm(chEditsByDate)
rm(chGroupAveMoPV)
rm(chOwnerAveMoPV)
rm(chOwnerPageCount)
rm(EditsByContentGr)
rm(EditsByDate)
rm(EditsBySteward)
rm(OwnerPageCount)
rm(scatEdits)



========================================================================================

## 6. Write out database load files to /finished/

http://r4ds.had.co.nz/data-import.html#writing-to-a-file

```{r}
write.csv(pageData, file = "finished/page.csv", na="")
write.csv(groupData, file = "finished/group.csv", na="")
```
