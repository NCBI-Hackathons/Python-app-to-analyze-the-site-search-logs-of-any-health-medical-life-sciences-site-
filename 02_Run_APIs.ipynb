{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Run APIs\n",
    "App to analyze web-site search logs (internal search)<br>\n",
    "**This script:** Match query entries against UMLS REST API<br>\n",
    "Authors: dan.wendling@nih.gov, <br>\n",
    "Last modified: 2018-09-09\n",
    "\n",
    "\n",
    "## Script contents\n",
    "\n",
    "Rather than re-using the same code during similar-and-optional runs, I duped and modified for special cases. Could be re-factored.\n",
    "\n",
    "1. Start-up\n",
    "2. UmlsApi1 - Normalized string matching\n",
    "3. Isolate entries updated by API, complete tagging, and match to the \n",
    "   current version of the search log - logAfterUmlsApi\n",
    "4. Create logAfterUmlsApi as an update to logAfterGoldStandard by appending \n",
    "   newUmlsWithSemanticGroupData\n",
    "5. Update GoldStandard\n",
    "6. Create new 'uniques' dataframe/file for fuzzy matching\n",
    "\n",
    "8. UmlsApi2 - Tag non-English terms in Roman character sets\n",
    "\n",
    "7. UmlsApi3 - Word matching (relax prediction rules )\n",
    "\n",
    "8. RxNorm API\n",
    "\n",
    "9. UmlsApi4 - Re-run first config - Create logAfterUmlsApi4 as an \n",
    "   update to logAfterUmlsApi by \n",
    "   \n",
    "   append newUmlsWithSemanticGroupData\n",
    "   \n",
    "10. Create updated training file (GoldStandard) for ML script\n",
    "\n",
    "Google Translate API, https://cloud.google.com/translate/\n",
    "But it's not free; https://stackoverflow.com/questions/37667671/is-it-possible-to-access-to-google-translate-api-for-free\n",
    "\n",
    "\n",
    "## FIXMEs\n",
    "\n",
    "Things Dan wrote for Dan; modify as needed. There are more FIXMEs in context.\n",
    "\n",
    "* [ ] Improve/clarify processing flow\n",
    "* [ ] Change SemanticNetworkReference.UniqueID to SemanticTypeCode\n",
    "* [ ] Add SemanticNetworkReference.SemanticTypeCode to what goes into the logs, for ML.\n",
    "\n",
    "\n",
    "## RESOURCES\n",
    "\n",
    "* Register at UMLS, get a UMLS-UTS API key, and add it below. This is the \n",
    "primary source for Semantic Type classifications.\n",
    "https://documentation.uts.nlm.nih.gov/rest/authentication.html\n",
    "* UMLS quick start: \n",
    "UMLS description of what Normalized String option is, \n",
    "https://uts.nlm.nih.gov/doc/devGuide/webservices/metaops/find/find2.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '01_Pre-processing_files/listOfUniqueUnassignedAfterGS.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-44823dadb110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# If you're starting a new session an this is not already open\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mlistOfUniqueUnassignedAfterGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'01_Pre-processing_files/listOfUniqueUnassignedAfterGS.xlsx'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mlistOfUniqueUnassignedAfterGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistOfUniqueUnassignedAfterGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Bring in historical file of (somewhat edited) matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, **kwds)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     return io.parse(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '01_Pre-processing_files/listOfUniqueUnassignedAfterGS.xlsx'"
     ]
    }
   ],
   "source": [
    "# 1. Start-up / What to put into place, where\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import pie, axis, show\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import lxml.html as lh\n",
    "from lxml.html import fromstring\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set working directory\n",
    "os.chdir('/Users/wendlingd/webDS')\n",
    "\n",
    "\n",
    "localDir = '02_Run_APIs_files/'\n",
    "\n",
    "# If you're starting a new session an this is not already open\n",
    "listOfUniqueUnassignedAfterGS = '01_Pre-processing_files/listOfUniqueUnassignedAfterGS.xlsx'\n",
    "listOfUniqueUnassignedAfterGS = pd.read_excel(listOfUniqueUnassignedAfterGS)\n",
    "\n",
    "# Bring in historical file of (somewhat edited) matches\n",
    "GoldStandard = '01_Pre-processing_files/GoldStandard_master.xlsx'\n",
    "GoldStandard = pd.read_excel(GoldStandard)\n",
    "\n",
    "\n",
    "\n",
    "# Get API key\n",
    "def get_umls_api_key(filename=None):\n",
    "    key = os.environ.get('UMLS_API_KEY', None)\n",
    "    if key is not None:\n",
    "        return key\n",
    "    if filename is None:\n",
    "          path = os.environ.get('HOME', None)\n",
    "          if path is None:\n",
    "               path = os.environ.get('USERPROFILE', None)\n",
    "          if path is None:\n",
    "               path = '.'\n",
    "          filename = os.path.join(path, '.umls_api_key')\n",
    "    with open(filename, 'r') as f:\n",
    "           key = f.readline().strip()\n",
    "    return key\n",
    "\n",
    "myUTSAPIkey = get_umls_api_key()\n",
    "\n",
    "\n",
    "'''\n",
    "GoldStandard.xlsx - Already-assigned term list, from UMLS and other sources, \n",
    "    vetted.\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "SemanticNetworkReference - Customized version of the list at \n",
    "https://www.nlm.nih.gov/research/umls/META3_current_semantic_types.html, \n",
    "to be used to put search terms into huge bins. Should be integrated into \n",
    "GoldStandard and be available at the end of the ML matching process.\n",
    "'''\n",
    "SemanticNetworkReference = '01_Pre-processing_files/SemanticNetworkReference.xlsx'\n",
    "\n",
    "\n",
    "''' \n",
    "- Run what remains against the UMLS API.\n",
    "\n",
    "Requires having your own license and API key; see https://www.nlm.nih.gov/research/umls/\n",
    "Not shown here: \n",
    "    - In huge files I sort by count and focus on terms searched by multiple\n",
    "    or many people. The 'long tail' can be huge.\n",
    "    - I have a database of terms aready assigned. I match these before \n",
    "    contacting UMLS; no need to check them again. Shortens processing time.\n",
    "More options:\n",
    "    https://documentation.uts.nlm.nih.gov/rest/home.html\n",
    "    https://documentation.uts.nlm.nih.gov/rest/concept/\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# unassignedAfterUmls1 = pd.read_excel(localdir + 'unassignedAfterUmls1.xlsx')\n",
    "\n",
    "'''\n",
    "Register at RxNorm, get API key, and add it below. This is for drug misspellings.\n",
    "'''\n",
    "\n",
    "# Generate a one-day Ticket-Granting-Ticket (TGT)\n",
    "tgt = requests.post('https://utslogin.nlm.nih.gov/cas/v1/api-key', data = {'apikey':myUTSAPIkey})\n",
    "# For API key get a license from https://www.nlm.nih.gov/research/umls/\n",
    "# tgt.text\n",
    "response = fromstring(tgt.text)\n",
    "todaysTgt = response.xpath('//form/@action')[0]\n",
    "\n",
    "uiUri = \"https://uts-ws.nlm.nih.gov/rest/search/current?\"\n",
    "semUri = \"https://uts-ws.nlm.nih.gov/rest/content/current/CUI/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. UmlsApi1 - Normalized string matching\n",
    "# =========================================\n",
    "'''\n",
    "In this run the API calls use the Normalized String setting. Example: \n",
    "for the input string Yellow leaves, normalizedString would return two strings, \n",
    "leaf yellow and leave yellow. Each string would be matched exactly to the \n",
    "strings in the normalized string index to return a result. \n",
    "\n",
    "Re-start:\n",
    "# listOfUniqueUnassignedAfterGS = pd.read_excel('01_Pre-processing_files/listOfUniqueUnassignedAfterGS.xlsx')\n",
    "\n",
    "listToCheck6 = pd.read_excel(localDir + 'listToCheck6.xlsx')\n",
    "listToCheck7 = pd.read_excel(localDir + 'listToCheck7.xlsx')\n",
    "'''\n",
    "\n",
    "# ---------------------------------------\n",
    "# Batch rows so you can do separate runs\n",
    "# Max of 5,000 rows per run\n",
    "# ---------------------------------------\n",
    "\n",
    "# uniqueSearchTerms = search['adjustedQueryCase'].unique()\n",
    "\n",
    "# Reduce entry length, to focus on single concepts that UTS API can match\n",
    "listOfUniqueUnassignedAfterGS = listOfUniqueUnassignedAfterGS.loc[(listOfUniqueUnassignedAfterGS['adjustedQueryCase'].str.len() <= 20) == True]\n",
    "\n",
    "\n",
    "# listToCheck1 = unassignedAfterGS.iloc[0:20]\n",
    "listToCheck1 = listOfUniqueUnassignedAfterGS.iloc[0:6000]\n",
    "listToCheck2 = listOfUniqueUnassignedAfterGS.iloc[6001:12000]\n",
    "listToCheck3 = listOfUniqueUnassignedAfterGS.iloc[12001:18000]\n",
    "listToCheck4 = listOfUniqueUnassignedAfterGS.iloc[18001:24000]\n",
    "listToCheck5 = listOfUniqueUnassignedAfterGS.iloc[24001:30000]\n",
    "listToCheck6 = listOfUniqueUnassignedAfterGS.iloc[30001:36000]\n",
    "listToCheck7 = listOfUniqueUnassignedAfterGS.iloc[36001:39523]\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "listToCheck1 = unassignedToCheck.iloc[12497:20000]\n",
    "listToCheck2 = unassignedToCheck.iloc[20001:26000]\n",
    "listToCheck3 = unassignedToCheck.iloc[23225:28000]\n",
    "listToCheck4 = unassignedToCheck.iloc[28001:31256]\n",
    "\n",
    "mask = (unassignedToCheck['adjustedQueryCase'].str.len() <= 15)\n",
    "listToCheck3 = listToCheck3.loc[mask]\n",
    "listToCheck4 = listToCheck4.loc[mask]\n",
    "'''\n",
    "\n",
    "\n",
    "# If multiple sessions required, saving to file might help\n",
    "writer = pd.ExcelWriter(localDir + 'listToCheck7.xlsx')\n",
    "listToCheck7.to_excel(writer,'listToCheck7')\n",
    "# df2.to_excel(writer,'Sheet2')\n",
    "writer.save()\n",
    "\n",
    "writer = pd.ExcelWriter(localDir + 'listToCheck2.xlsx')\n",
    "listToCheck2.to_excel(writer,'listToCheck2')\n",
    "# df2.to_excel(writer,'Sheet2')\n",
    "writer.save()\n",
    "\n",
    "'''\n",
    "OPTIONS\n",
    "\n",
    "# Bring in from file\n",
    "listToCheck3 = pd.read_excel(localDir + 'listToCheck3.xlsx')\n",
    "listToCheck4 = pd.read_excel(localDir + 'listToCheck4.xlsx')\n",
    "\n",
    "listToCheck1 = unassignedAfterGS\n",
    "listToCheck2 = unassignedAfterGS.iloc[5001:10000]\n",
    "listToCheck1 = unassignedAfterGS.iloc[10001:11335]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block after changing listToCheck# top and bottom\n",
    "# ----------------------------------------------------------\n",
    "'''\n",
    "Until you put this into a function, you need to change listToCheck# \n",
    "and apiGetNormalizedString# counts every run!\n",
    "Stay below 30 API requests per second. With 4 API requests per item\n",
    "(2 .get and 2 .post requests)...\n",
    "time.sleep commented out: 6,000 / 35 min = 171 per minute = 2.9 items per second / 11.4 requests per second\n",
    "Computing differently, 6,000 items @ 4 Req per item = 24,000 Req, divided by 35 min+\n",
    "686 Req/min = 11.4 Req/sec\n",
    "time.sleep(.07):  ~38 minutes to do 6,000; 158 per minute / 2.6 items per second\n",
    "'''\n",
    "\n",
    "apiGetNormalizedString = pd.DataFrame()\n",
    "apiGetNormalizedString['adjustedQueryCase'] = \"\"\n",
    "apiGetNormalizedString['preferredTerm'] = \"\"\n",
    "apiGetNormalizedString['SemanticTypeName'] = \"\"\n",
    "\n",
    "'''\n",
    "For file 6, 7/5/18 1:05 p.m.: SSLError: HTTPSConnectionPool(host='utslogin.nlm.nih.gov', \n",
    "port=443): Max retries exceeded with url: \n",
    "/cas/v1/api-key/TGT-480224-qLwYAMKl5cTfa7Jwb7RWZ3kfexPUm479HfddD7yVUKt79lZ0Ta-cas \n",
    "(Caused by SSLError(SSLError(\"bad handshake: SysCallError(60, 'ETIMEDOUT')\",),))\n",
    "\n",
    "Later, run 6 and 7\n",
    "'''\n",
    "\n",
    "\n",
    "for index, row in listToCheck7.iterrows():\n",
    "    currLogTerm = row['adjustedQueryCase']\n",
    "    # === Get 'preferred term' and its concept identifier (CUI/UI) =========\n",
    "    stTicket = requests.post(todaysTgt, data = {'service':'http://umlsks.nlm.nih.gov'}) # Get single-use Service Ticket (ST)\n",
    "    # Example: GET https://uts-ws.nlm.nih.gov/rest/search/current?string=tylenol&sabs=MSH&ticket=ST-681163-bDfgQz5vKe2DJXvI4Snm-cas\n",
    "    tQuery = {'string':currLogTerm, 'searchType':'normalizedString', 'ticket':stTicket.text} # removed 'sabs':'MSH', \n",
    "    getPrefTerm = requests.get(uiUri, params=tQuery)\n",
    "    getPrefTerm.encoding = 'utf-8'\n",
    "    tItems  = json.loads(getPrefTerm.text)\n",
    "    tJson = tItems[\"result\"]\n",
    "    if tJson[\"results\"][0][\"ui\"] != \"NONE\": # Sub-loop to resolve \"NONE\"\n",
    "        currUi = tJson[\"results\"][0][\"ui\"]\n",
    "        currPrefTerm = tJson[\"results\"][0][\"name\"]\n",
    "        # === Get 'semantic type' =========\n",
    "        stTicket = requests.post(todaysTgt, data = {'service':'http://umlsks.nlm.nih.gov'}) # Get single-use Service Ticket (ST)\n",
    "        # Example: GET https://uts-ws.nlm.nih.gov/rest/content/current/CUI/C0699142?ticket=ST-512564-vUxzyI00ErMRm6tjefNP-cas\n",
    "        semQuery = {'ticket':stTicket.text}\n",
    "        getPrefTerm = requests.get(semUri+currUi, params=semQuery)\n",
    "        getPrefTerm.encoding = 'utf-8'\n",
    "        semItems  = json.loads(getPrefTerm.text)\n",
    "        semJson = semItems[\"result\"]\n",
    "        currSemTypes = []\n",
    "        for name in semJson[\"semanticTypes\"]:\n",
    "            currSemTypes.append(name[\"name\"]) #  + \" ; \"\n",
    "        # === Post to dataframe =========\n",
    "        apiGetNormalizedString = apiGetNormalizedString.append(pd.DataFrame({'adjustedQueryCase': currLogTerm, \n",
    "                                                       'preferredTerm': currPrefTerm, \n",
    "                                                       'SemanticTypeName': currSemTypes[0]}, index=[0]), ignore_index=True)\n",
    "        print('{} --> {}'.format(currLogTerm, currSemTypes[0])) # Write progress to console\n",
    "        # time.sleep(.06)\n",
    "    else:\n",
    "       # Post \"NONE\" to database and restart loop\n",
    "        apiGetNormalizedString = apiGetNormalizedString.append(pd.DataFrame({'adjustedQueryCase': currLogTerm, 'preferredTerm': \"NONE\"}, index=[0]), ignore_index=True)\n",
    "        print('{} --> NONE'.format(currLogTerm, )) # Write progress to console\n",
    "        # time.sleep(.06)\n",
    "print (\"* Done *\")\n",
    "\n",
    "\n",
    "writer = pd.ExcelWriter(localDir + 'apiGetNormalizedString7.xlsx')\n",
    "apiGetNormalizedString.to_excel(writer,'apiGetNormalizedString')\n",
    "# df2.to_excel(writer,'Sheet2')\n",
    "writer.save()\n",
    "\n",
    "\n",
    "# Free up memory: Remove listToCheck, listToCheck1, listToCheck2, listToCheck3, \n",
    "# listToCheck4, nonForeign, searchLog, unassignedAfterGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Isolate entries updated by API, complete tagging, and match to \n",
    "# the current version of the search log - logAfterUmlsApi\n",
    "# ==================================================================\n",
    "'''\n",
    "To Do:\n",
    "\n",
    "    Isolate new assignments and:\n",
    "    - merge them into the master version of the log\n",
    "    - add to GoldStandard for next time\n",
    " \n",
    " # Move unassigned entries into workflow for human identification\n",
    " \n",
    "To re-start\n",
    "\n",
    "unassignedAfterGS = pd.read_excel(localDir + 'unassignedAfterGS.xlsx')\n",
    "logAfterGoldStandard = pd.read_excel(localDir + 'logAfterGoldStandard.xlsx')\n",
    "\n",
    "listFromApi = pd.read_excel('02_UMLS_API_files/listFromApi1-April-May.xlsx')\n",
    "assignedByUmlsApi = pd.read_excel(localDir + 'assignedByUmlsApi.xlsx')\n",
    "\n",
    "# Fix temporary issue of nulls in SemanticTypeName, and wrong col name semTypeName\n",
    " \n",
    "listFromApi.drop(['SemanticTypeName'], axis=1, inplace=True)\n",
    "listFromApi.rename(columns={'semTypeName': 'SemanticTypeName'}, inplace=True)\n",
    "\n",
    "# listFromApi = listFromApi.dropna(subset=['SemanticTypeName'])\n",
    " '''\n",
    " \n",
    "\n",
    "# If you stored output from UMLS API in files, re-open and unite\n",
    "newAssignments1 = pd.read_excel(localDir + 'apiGetNormalizedString1.xlsx')\n",
    "newAssignments2 = pd.read_excel(localDir + 'apiGetNormalizedString2.xlsx')\n",
    "newAssignments3 = pd.read_excel(localDir + 'apiGetNormalizedString3.xlsx')\n",
    "newAssignments4 = pd.read_excel(localDir + 'apiGetNormalizedString4.xlsx')\n",
    "newAssignments5 = pd.read_excel(localDir + 'apiGetNormalizedString5.xlsx')\n",
    "newAssignments6 = pd.read_excel(localDir + 'apiGetNormalizedString6.xlsx')\n",
    "newAssignments7 = pd.read_excel(localDir + 'apiGetNormalizedString7.xlsx')\n",
    "\n",
    "\n",
    "# Put dataframes together into one; df = df1.append([df2, df3])\n",
    "afterUmlsApi1 = newAssignments1.append([newAssignments2, newAssignments3, newAssignments4, newAssignments5])\n",
    "afterUmlsApi1 = newAssignments6.append([newAssignments7])\n",
    "\n",
    "\n",
    "'''\n",
    "afterUmlsApi1 = afterUmlsApi1.append(newAssignments3)\n",
    "afterUmlsApi1 = afterUmlsApi1.append(newAssignments4)\n",
    "'''\n",
    "\n",
    "\n",
    "# If you only used one df for listFromApi\n",
    "# afterUMLSapi = listFromApi\n",
    "# assignedByUmlsApi = listFromApi\n",
    "\n",
    "\n",
    "# Reduce to a version that has only successful assignments\n",
    "\n",
    "# Remove various problem entries\n",
    "assignedByUmlsApi1 = afterUmlsApi1.loc[(afterUmlsApi1['preferredTerm'] != \"NONE\")]\n",
    "assignedByUmlsApi1 = assignedByUmlsApi1[~pd.isnull(assignedByUmlsApi1['preferredTerm'])]\n",
    "assignedByUmlsApi1 = assignedByUmlsApi1.loc[(assignedByUmlsApi1['preferredTerm'] != \"Null Value\")]\n",
    "assignedByUmlsApi1 = assignedByUmlsApi1[~pd.isnull(assignedByUmlsApi1['adjustedQueryCase'])]\n",
    "\n",
    "\n",
    "# If you want to send to Excel\n",
    "writer = pd.ExcelWriter(localDir + 'assignedByUmlsApi1.xlsx')\n",
    "assignedByUmlsApi1.to_excel(writer,'assignedByUmlsApi1')\n",
    "# df2.to_excel(writer,'Sheet2')\n",
    "writer.save()\n",
    "\n",
    "\n",
    "# Bring in subject category master file\n",
    "# SemanticNetworkReference = pd.read_excel(localDir + 'SemanticNetworkReference.xlsx')\n",
    "SemanticNetworkReference = pd.read_excel(SemanticNetworkReference)\n",
    "\n",
    "# Reduce to required cols\n",
    "SemTypeData = SemanticNetworkReference[['SemanticTypeName', 'SemanticGroupCode', 'SemanticGroup', 'CustomTreeNumber', 'BranchPosition']]\n",
    "# SemTypeData.rename(columns={'SemanticTypeName': 'semTypeName'}, inplace=True) # The join col\n",
    "\n",
    "# Add more semantic tagging to new UMLS API adds\n",
    "newUmlsWithSemanticGroupData = pd.merge(assignedByUmlsApi1, SemTypeData, how='left', on='SemanticTypeName')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create logAfterUmlsApi as an update to logAfterGoldStandard by appending \n",
    "# newUmlsWithSemanticGroupData\n",
    "# ============================================================================\n",
    "\n",
    "'''\n",
    "Depending on what you're processing, use this or the next section of the below.\n",
    "\n",
    "Depends on how you choose to process - Like, down to one occurrence to API \n",
    "in first batch, or not.\n",
    "'''\n",
    "\n",
    "\n",
    "logAfterGoldStandard = '01_Pre-processing_files/logAfterGoldStandard.xlsx'\n",
    "logAfterGoldStandard = pd.read_excel(logAfterGoldStandard)\n",
    "\n",
    "\n",
    "'''\n",
    "# FIXME - Remove after this is fixed within the fixme above.\n",
    "logAfterGoldStandard = logAfterGoldStandard.sort_values(by='adjustedQueryCase', ascending=True)\n",
    "logAfterGoldStandard = logAfterGoldStandard.reset_index()\n",
    "logAfterGoldStandard.drop(['index'], axis=1, inplace=True)\n",
    "'''\n",
    "\n",
    "\n",
    "# Eyeball. If you need to remove rows...\n",
    "# logAfterGoldStandard = logAfterGoldStandard.iloc[760:] # remove before index...\n",
    "\n",
    "# Join new UMLS API adds to the current search log master\n",
    "logAfterUmlsApi1 = pd.merge(logAfterGoldStandard, newUmlsWithSemanticGroupData, how='left', on='adjustedQueryCase')\n",
    "\n",
    "logAfterUmlsApi1.columns\n",
    "\n",
    "'''\n",
    "['SessionID', 'StaffYN', 'Referrer', 'Query', 'Timestamp',\n",
    "       'adjustedQueryCase', 'SemanticTypeName_x', 'SemanticGroup_x',\n",
    "       'SemanticGroupCode_x', 'BranchPosition_x', 'CustomTreeNumber_x',\n",
    "       'ResourceType', 'Address', 'EntrySource', 'contentSteward',\n",
    "       'preferredTerm_x', 'SemanticTypeName_y', 'preferredTerm_y',\n",
    "       'SemanticGroupCode_y', 'SemanticGroup_y', 'CustomTreeNumber_y',\n",
    "       'BranchPosition_y']\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Future: Look for a better way to do the above - MERGE WITH CONDITIONAL OVERWRITE. Temporary fix:\n",
    "logAfterUmlsApi1['preferredTerm2'] = logAfterUmlsApi1['preferredTerm_x'].where(logAfterUmlsApi1['preferredTerm_x'].notnull(), logAfterUmlsApi1['preferredTerm_y'])\n",
    "logAfterUmlsApi1['SemanticTypeName2'] = logAfterUmlsApi1['SemanticTypeName_x'].where(logAfterUmlsApi1['SemanticTypeName_x'].notnull(), logAfterUmlsApi1['SemanticTypeName_y'])\n",
    "logAfterUmlsApi1['SemanticGroup2'] = logAfterUmlsApi1['SemanticGroup_x'].where(logAfterUmlsApi1['SemanticGroup_x'].notnull(), logAfterUmlsApi1['SemanticGroup_y'])\n",
    "logAfterUmlsApi1['SemanticGroupCode2'] = logAfterUmlsApi1['SemanticGroupCode_x'].where(logAfterUmlsApi1['SemanticGroupCode_x'].notnull(), logAfterUmlsApi1['SemanticGroupCode_y'])\n",
    "logAfterUmlsApi1['BranchPosition2'] = logAfterUmlsApi1['BranchPosition_x'].where(logAfterUmlsApi1['BranchPosition_x'].notnull(), logAfterUmlsApi1['BranchPosition_y'])\n",
    "logAfterUmlsApi1['CustomTreeNumber2'] = logAfterUmlsApi1['CustomTreeNumber_x'].where(logAfterUmlsApi1['CustomTreeNumber_x'].notnull(), logAfterUmlsApi1['CustomTreeNumber_y'])\n",
    "logAfterUmlsApi1.drop(['preferredTerm_x', 'preferredTerm_y',\n",
    "                          'SemanticTypeName_x', 'SemanticTypeName_y',\n",
    "                          'SemanticGroup_x', 'SemanticGroup_y',\n",
    "                          'SemanticGroupCode_x', 'SemanticGroupCode_y',\n",
    "                          'BranchPosition_x', 'BranchPosition_y', \n",
    "                          'CustomTreeNumber_x', 'CustomTreeNumber_y'], axis=1, inplace=True)\n",
    "logAfterUmlsApi1.rename(columns={'preferredTerm2': 'preferredTerm',\n",
    "                                    'SemanticTypeName2': 'SemanticTypeName',\n",
    "                                    'SemanticGroup2': 'SemanticGroup',\n",
    "                                    'SemanticGroupCode2': 'SemanticGroupCode',\n",
    "                                    'BranchPosition2': 'BranchPosition',\n",
    "                                    'CustomTreeNumber2': 'CustomTreeNumber'\n",
    "                                    }, inplace=True)\n",
    "\n",
    "# Save to file so you can open in future sessions, if needed\n",
    "writer = pd.ExcelWriter(localDir + 'logAfterUmlsApi1.xlsx')\n",
    "logAfterUmlsApi1.to_excel(writer,'logAfterUmlsApi1')\n",
    "# df2.to_excel(writer,'Sheet2')\n",
    "writer.save()\n",
    "\n",
    "'''\n",
    "To Do:\n",
    "    - Create list of unmatched terms with freq\n",
    "    - Cluster similar spellings together?\n",
    "    \n",
    "- Look at \"Not currently matchable\" terms with \"high\" frequency counts. Eyeball to see if these were incorrectly matched in the past; assign historical term or update all to new term, save in gold standard file.\n",
    "- Process entries from the PubMed product page.\n",
    "- If you haven't done so, update RegEx list to improve future matching.\n",
    "- Every several months, through Flask interface, interactively update the gold standard, manually.\n",
    "\n",
    "# Reduce logAfterUmlsApi to unique, unmatched entries, prep for ML\n",
    "\n",
    "To re-start:\n",
    "logAfterUmlsApi = pd.read_excel(localDir + 'logAfterUmlsApi.xlsx')\n",
    "'''\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Visualize results - logAfterUmlsApi\n",
    "# ------------------------------------\n",
    "    \n",
    "# Pie for percentage of rows assigned; https://pythonspot.com/matplotlib-pie-chart/\n",
    "totCount = len(logAfterUmlsApi1)\n",
    "unassigned = logAfterUmlsApi1['SemanticGroup'].isnull().sum()\n",
    "assigned = totCount - unassigned\n",
    "labels = ['Assigned', 'Unassigned']\n",
    "sizes = [assigned, unassigned]\n",
    "colors = ['lightskyblue', 'lightcoral']\n",
    "explode = (0.1, 0)  # explode 1st slice\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.f%%', shadow=True, startangle=100)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Status after 'UMLS API' processing\")\n",
    "plt.show()\n",
    "\n",
    "# Bar of SemanticGroup categories, horizontal\n",
    "# Source: http://robertmitchellv.com/blog-bar-chart-annotations-pandas-mpl.html\n",
    "ax = logAfterUmlsApi1['SemanticGroup'].value_counts().plot(kind='barh', figsize=(10,6),\n",
    "                                                 color=\"slateblue\", fontsize=10);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"Categories assigned after 'UMLS API' processing\", fontsize=14)\n",
    "ax.set_xlabel(\"Number of searches\", fontsize=9);\n",
    "# set individual bar lables using above list\n",
    "for i in ax.patches:\n",
    "    # get_width pulls left or right; get_y pushes up or down\n",
    "    ax.text(i.get_width()+.1, i.get_y()+.31, \\\n",
    "            str(round((i.get_width()), 2)), fontsize=9, color='dimgrey')\n",
    "# invert for largest on top \n",
    "ax.invert_yaxis()\n",
    "plt.gcf().subplots_adjust(left=0.3)\n",
    "\n",
    "# Remove listOfUniqueUnassignedAfterGS, listToCheck1, etc., logAfterGoldStandard, logAfterUmlsApi1, \n",
    "# newAssignments1 etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Update GoldStandard\n",
    "# =======================\n",
    "\n",
    "# Open GoldStandard if needed\n",
    "GoldStandard = '01_Pre-processing_files/GoldStandard.xlsx'\n",
    "GoldStandard = pd.read_excel(GoldStandard)\n",
    "\n",
    "# Append fully tagged UMLS API adds to GoldStandard\n",
    "GoldStandard = GoldStandard.append(newUmlsWithSemanticGroupData, sort=False)\n",
    "\n",
    "# Reset index\n",
    "GoldStandard = GoldStandard.reset_index()\n",
    "GoldStandard.drop(['index'], axis=1, inplace=True)\n",
    "# temp GoldStandard.drop(['adjustedQueryCase'], axis=1, inplace=True)\n",
    "\n",
    "'''\n",
    "Eyeball top and bottom of cols, remove rows by Index, if needed\n",
    "\n",
    "GoldStandard.drop(58027, inplace=True)\n",
    "'''\n",
    "\n",
    "\n",
    "# Write out the updated GoldStandard\n",
    "writer = pd.ExcelWriter('01_Pre-processing_files/GoldStandard.xlsx')\n",
    "GoldStandard.to_excel(writer,'GoldStandard')\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Start new 'uniques' dataframe that gets new column for each of the below\n",
    "# listOfUniqueUnassignedAfterUmls1\n",
    "# ============================================================================\n",
    "\n",
    "'''\n",
    "To Do:\n",
    "    - Create list of unmatched terms with freq\n",
    "    - Cluster similar spellings together?\n",
    "    \n",
    "- Look at \"Not currently matchable\" terms with \"high\" frequency counts. Eyeball to see if these were incorrectly matched in the past; assign historical term or update all to new term, save in gold standard file.\n",
    "- Process entries from the PubMed product page.\n",
    "- If you haven't done so, update RegEx list to improve future matching.\n",
    "- Every several months, through Flask interface, interactively update the gold standard, manually.\n",
    "\n",
    "# Reduce logAfterUmlsApi to unique, unmatched entries, prep for ML\n",
    "\n",
    "To re-start:\n",
    "logAfterUmlsApi = pd.read_excel(localDir + 'logAfterUmlsApi.xlsx')\n",
    "'''\n",
    "\n",
    "listOfUniqueUnassignedAfterUmls1 = logAfterUmlsApi1[pd.isnull(logAfterUmlsApi1['SemanticGroup'])]\n",
    "listOfUniqueUnassignedAfterUmls1 = listOfUniqueUnassignedAfterUmls1.groupby('adjustedQueryCase').size()\n",
    "listOfUniqueUnassignedAfterUmls1 = pd.DataFrame({'timesSearched':listOfUniqueUnassignedAfterUmls1})\n",
    "listOfUniqueUnassignedAfterUmls1 = listOfUniqueUnassignedAfterUmls1.sort_values(by='timesSearched', ascending=False)\n",
    "listOfUniqueUnassignedAfterUmls1 = listOfUniqueUnassignedAfterUmls1.reset_index()\n",
    "\n",
    "writer = pd.ExcelWriter(localDir + 'listOfUniqueUnassignedAfterUmls11.xlsx')\n",
    "listOfUniqueUnassignedAfterUmls1.to_excel(writer,'unassignedToCheck')\n",
    "writer.save()\n",
    "\n",
    "# FY 18 Q3: 57,287\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Google Translate API, https://cloud.google.com/translate/\n",
    "# =============================================================\n",
    "'''\n",
    "But it's not free; https://stackoverflow.com/questions/37667671/is-it-possible-to-access-to-google-translate-api-for-free\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. UmlsApi2 - Tag non-English terms in Roman character sets\n",
    "# ==========================================================================\n",
    "'''\n",
    "Some foreign terms can be matched. This run does not return a preferred term,\n",
    "just returns what vocabulary the term is found in. \n",
    "\n",
    "Queries with words not in English are ignored by the first API run using\n",
    "\"normalized string\" matching. Here, try flagging what you can and take them \n",
    "out of the percent-complete calculation.\n",
    "\n",
    "The API apparently only supports U.S. English. RegEx could be used to convert\n",
    "UTF-8 Roman characters that are not English... Non-Roman languages (Chinese, \n",
    "Cyrillic, Arabic, Japanese, etc.) are not supported by the API; these should \n",
    "be kept out of the API runs entirely.\n",
    "\n",
    "6/22/18, from David of UMLS support, TRACKING:000308010\n",
    "\n",
    "> Can the UMLS REST API tell me the term's language? \n",
    "\n",
    "One option would be to specify returnIdType=sourceUi for your search \n",
    "request. For example: \n",
    " \n",
    "https://uts-ws.nlm.nih.gov/rest/search/current?string=Infarto de miocardio&returnIdType=sourceUi&ticket=\n",
    "\n",
    "This will give you a set of codes back where there is a match, but will \n",
    "also return a vocabulary (rootSource). If you have that, you can get \n",
    "the language (in this case, Spanish). The first result may be all you \n",
    "need. If you have the rootSource, you can match it to the \"abbreviation\" \n",
    "and look up the language here: https://uts-ws.nlm.nih.gov/rest/metadata/current/sources. \n",
    " \n",
    "It won't be perfect. I'm seeing some problems with accented characters. \n",
    "For example, coração returns no results, so that's not great, but may \n",
    "not matter. Some strings will appear in multiple languages, too. \n",
    " \n",
    "Let me know how that works for you. - David\n",
    "'''\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Batch up your API runs. Re-starting, correcting, etc.\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# uniqueSearchTerms = search['adjustedQueryCase'].unique()\n",
    "\n",
    "# vocabCheck1 = unassignedAfterGS.iloc[0:20]\n",
    "vocabCheck1 = listOfUniqueUnassignedAfterUmls1.iloc[0:5000]\n",
    "# vocabCheck2 = listOfUniqueUnassignedAfterUmls1.iloc[5001:10678]\n",
    "\n",
    "\n",
    "# If multiple sessions required, saving to file might help\n",
    "writer = pd.ExcelWriter(localDir + 'vocabCheck1.xlsx')\n",
    "vocabCheck1.to_excel(writer,'vocabCheck')\n",
    "# df2.to_excel(writer,'Sheet2')\n",
    "writer.save()\n",
    "\n",
    "\n",
    "'''\n",
    "writer = pd.ExcelWriter(localDir + 'listToCheck2.xlsx')\n",
    "listToCheck2.to_excel(writer,'listToCheck2')\n",
    "# df2.to_excel(writer,'Sheet2')\n",
    "writer.save()\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "OPTIONS\n",
    "\n",
    "# Bring in from file\n",
    "listToCheck3 = pd.read_excel('01 Pre-process/listToCheck3.xlsx')\n",
    "listToCheck4 = pd.read_excel('01 Pre-process/listToCheck4.xlsx')\n",
    "\n",
    "listToCheck1 = unassignedAfterGS\n",
    "listToCheck2 = unassignedAfterGS.iloc[5001:10000]\n",
    "listToCheck1 = unassignedAfterGS.iloc[10001:11335]\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "Work with PostMan app to test/approve\n",
    "\n",
    "David from UMLS: One option would be to specify returnIdType=sourceUi for \n",
    "your search request. For example:\n",
    " \n",
    "https://uts-ws.nlm.nih.gov/rest/search/current?string=Infarto de miocardio&returnIdType=sourceUi&ticket=\n",
    "\n",
    "This will give you a set of codes back where there is a match, but will \n",
    "also return a vocabulary (rootSource). If you have that, you can get the \n",
    "language (in this case, Spanish). The first result may be all you need. \n",
    "If you have the rootSource, you can match it to the \"abbreviation\" and \n",
    "look up the language here: https://uts-ws.nlm.nih.gov/rest/metadata/current/sources. \n",
    " \n",
    "It won't be perfect. I'm seeing some problems with accented characters. \n",
    "For example, coração returns no results, so that's not great, but may \n",
    "not matter. Some strings will appear in multiple languages, too. \n",
    " \n",
    "Let me know how that works for you.\n",
    "'''\n",
    "\n",
    "'''\n",
    "FIXME - Unfinished.\n",
    "\n",
    "TGT-16294-ajZgfOTNGBxvzAXAvQslZtuL2U0HksFsED6tZ0ajoewNBNdSVz-cas\n",
    "\n",
    "\n",
    "# THIS IS SOURCE VOCAB CODE\n",
    "\n",
    "https://uts-ws.nlm.nih.gov/rest/search/current?string=Infarto de miocardio&returnIdType=sourceUi&ticket=\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather list of source vocabularies\n",
    "# -----------------------------------\n",
    "\n",
    "uiUri = \"https://uts-ws.nlm.nih.gov/rest/search/current?returnIdType=sourceUi\"\n",
    "\n",
    "listOfSourceVocabularies = pd.DataFrame()\n",
    "listOfSourceVocabularies['adjustedQueryCase'] = \"\"\n",
    "listOfSourceVocabularies['sourceVocab'] = \"\"\n",
    "\n",
    "for index, row in listToCheck1.iterrows():\n",
    "    currLogTerm = row['adjustedQueryCase']\n",
    "    # === Get 'source vocab' =========\n",
    "    stTicket = requests.post(todaysTgt, data = {'service':'http://umlsks.nlm.nih.gov'}) # Get single-use Service Ticket (ST)\n",
    "    # Example: GET https://uts-ws.nlm.nih.gov/rest/search/current?string=tylenol&sabs=MSH&ticket=ST-681163-bDfgQz5vKe2DJXvI4Snm-cas\n",
    "    termQuery = {'string':currLogTerm, 'ticket':stTicket.text} # removed 'searchType':'word' (it's the default),      'sabs':'MSH', \n",
    "    getSourceVocab = requests.get(uiUri, params=termQuery)\n",
    "    getSourceVocab.encoding = 'utf-8'\n",
    "    tItems = json.loads(getSourceVocab.text)\n",
    "    tJson = tItems[\"result\"]\n",
    "    if tJson[\"results\"][0][\"ui\"] != \"NONE\": # Sub-loop to resolve \"NONE\"\n",
    "        currUi = tJson[\"results\"][0][\"rootSource\"]\n",
    "        sourceVocab = tJson[\"results\"][0][\"rootSource\"]\n",
    "        # === Post to dataframe =========\n",
    "        listOfSourceVocabularies = listOfSourceVocabularies.append(pd.DataFrame({'adjustedQueryCase': currLogTerm, \n",
    "                                                       'sourceVocab': sourceVocab}, index=[0]), ignore_index=True)\n",
    "        print('{} --> {}'.format(currLogTerm, sourceVocab)) # Write progress to console\n",
    "        time.sleep(.07)\n",
    "    else:\n",
    "       # Post \"NONE\" to database and restart loop\n",
    "        listOfSourceVocabularies = listOfSourceVocabularies.append(pd.DataFrame({'adjustedQueryCase': currLogTerm, 'sourceVocab': \"NONE\"}, index=[0]), ignore_index=True)\n",
    "        print('{} --> NONE'.format(currLogTerm, )) # Write progress to console\n",
    "        time.sleep(.07)\n",
    "print (\"* Done *\")\n",
    "\n",
    "\n",
    "writer = pd.ExcelWriter(localDir + 'listOfSourceVocabularies.xlsx')\n",
    "listOfSourceVocabularies.to_excel(writer,'listOfSourceVocabularies')\n",
    "# df2.to_excel(writer,'Sheet2')\n",
    "writer.save()\n",
    "\n",
    "# Free up memory: Remove listToCheck, listToCheck1, listToCheck2, listToCheck3, \n",
    "# listToCheck4, nonForeign, searchLog, unassignedAfterGS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load external reference file: SourceVocabsForeign.xlsx\n",
    "\n",
    "# F&R Foreign vocab names with the language name, \"Spanish,\" \"Swedish\"\n",
    "\n",
    "# Append to running list of updates\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Match vocabCheck \n",
    "# ------------------------------------------------------\n",
    "'''\n",
    "FIXME - RESULTING LIST NEEDS TO BE VETTED; START WITH HIGHEST-FREQUENCY USE.\n",
    "\n",
    "Update naming? this is the result from the API run for languages\n",
    "\n",
    "This custom list of vocabs does not include and English vocabs, therefore, \n",
    "only foreign matches are returned, which is what we want.\n",
    "\n",
    "Re-start:\n",
    "listOfSourceVocabularies = pd.read_excel(localDir + 'listOfSourceVocabularies.xlsx')\n",
    "'''\n",
    "\n",
    "# Load list of Non-English vocabularies\n",
    "# 7/5/2018, https://www.nlm.nih.gov/research/umls/sourcereleasedocs/index.html (English vocabs not included.)\n",
    "UMLS_NonEnglish_Vocabularies = pd.read_excel(localDir + 'UMLS_Non-English_Vocabularies.xlsx')\n",
    "\n",
    "# Inner join\n",
    "foreignButEnglishChar = pd.merge(listOfSourceVocabularies, UMLS_NonEnglish_Vocabularies, how='inner', left_on='sourceVocab', right_on='Vocabulary')\n",
    "\n",
    "\n",
    "# Get frequency count, reduce cols for easier manual checking\n",
    "PerhapsForeign = pd.merge(foreignButEnglishChar, listOfUniqueUnassignedAfterUmls1, how='inner', on='adjustedQueryCase')\n",
    "\n",
    "PerhapsForeign = PerhapsForeign.sort_values(by='timesSearched', ascending=False)\n",
    "PerhapsForeign = PerhapsForeign.reset_index()\n",
    "PerhapsForeign.drop(['index'], axis=1, inplace=True)\n",
    "col = ['adjustedQueryCase', 'timesSearched', 'Language']\n",
    "PerhapsForeign = PerhapsForeign[col]\n",
    "PerhapsForeign.rename(columns={'Language': 'LanguageGuess'}, inplace=True)\n",
    "\n",
    "# Send out for manual checking\n",
    "writer = pd.ExcelWriter(localDir + 'PerhapsForeign.xlsx')\n",
    "PerhapsForeign.to_excel(writer,'PerhapsForeign')\n",
    "# df2.to_excel(writer,'Sheet2')\n",
    "writer.save()\n",
    "\n",
    "'''\n",
    "In Excel or Flask, delete rows with terms that we use in English; check that \n",
    "tyhe remaining rows contain terms that most English speakers would think are \n",
    "foriegn. \n",
    "Supplement cols for the definite foreign terms, append to GoldStandard as \n",
    "foreign terms.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update GoldStandard with edits from PerhapsForeign result\n",
    "\n",
    "# Update current log file from PerhapsForeign result\n",
    "\n",
    "# Create new 'uniques' list for FuzzyWuzzy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Second UMLS API clean-up - Create logAfterUmlsApi2 as an \n",
    "# update to logAfterUmlsApi by appending newUmlsWithSemanticGroupData\n",
    "# ===========================================================================\n",
    "'''\n",
    "Use this AFTER you do a SECOND run against the UMLS Metathesaurus API.\n",
    "\n",
    "Re-start: \n",
    "logAfterUmlsApi2 = pd.read_excel(localDir + 'logAfterUmlsApi2.xlsx')\n",
    "'''\n",
    "\n",
    "logAfterUmlsApi2 = pd.read_excel(localDir + 'logAfterUmlsApi1.xlsx')\n",
    "\n",
    "# FIXME - Remove after this is fixed within the fixme above.\n",
    "logAfterUmlsApi2 = logAfterUmlsApi2.sort_values(by='adjustedQueryCase', ascending=False)\n",
    "logAfterUmlsApi2 = logAfterUmlsApi2.reset_index()\n",
    "logAfterUmlsApi2.drop(['index'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Join new UMLS API adds to the current search log master\n",
    "logAfterUmlsApi2 = pd.merge(logAfterUmlsApi, newUmlsWithSemanticGroupData, how='left', on='adjustedQueryCase')\n",
    "\n",
    "# Future: Look for a better way to do the above - MERGE WITH CONDITIONAL OVERWRITE. Temporary fix:\n",
    "logAfterUmlsApi2['preferredTerm2'] = logAfterUmlsApi2['preferredTerm_x'].where(logAfterUmlsApi2['preferredTerm_x'].notnull(), logAfterUmlsApi2['preferredTerm_y'])\n",
    "logAfterUmlsApi2['SemanticTypeName2'] = logAfterUmlsApi2['SemanticTypeName_x'].where(logAfterUmlsApi2['SemanticTypeName_x'].notnull(), logAfterUmlsApi2['SemanticTypeName_y'])\n",
    "logAfterUmlsApi2['SemanticGroupCode2'] = logAfterUmlsApi2['SemanticGroupCode_x'].where(logAfterUmlsApi2['SemanticGroupCode_x'].notnull(), logAfterUmlsApi2['SemanticGroupCode_y'])\n",
    "logAfterUmlsApi2['SemanticGroup2'] = logAfterUmlsApi2['SemanticGroup_x'].where(logAfterUmlsApi2['SemanticGroup_x'].notnull(), logAfterUmlsApi2['SemanticGroup_y'])\n",
    "logAfterUmlsApi2['BranchPosition2'] = logAfterUmlsApi2['BranchPosition_x'].where(logAfterUmlsApi2['BranchPosition_x'].notnull(), logAfterUmlsApi2['BranchPosition_y'])\n",
    "logAfterUmlsApi2['CustomTreeNumber2'] = logAfterUmlsApi2['CustomTreeNumber_x'].where(logAfterUmlsApi2['CustomTreeNumber_x'].notnull(), logAfterUmlsApi2['CustomTreeNumber_y'])\n",
    "logAfterUmlsApi2.drop(['preferredTerm_x', 'preferredTerm_y',\n",
    "                          'SemanticTypeName_x', 'SemanticTypeName_y',\n",
    "                          'SemanticGroup_x', 'SemanticGroup_y',\n",
    "                          'SemanticGroupCode_x', 'SemanticGroupCode_y',\n",
    "                          'BranchPosition_x', 'BranchPosition_y', \n",
    "                          'CustomTreeNumber_x', 'CustomTreeNumber_y'], axis=1, inplace=True)\n",
    "logAfterUmlsApi2.rename(columns={'preferredTerm2': 'preferredTerm',\n",
    "                                    'SemanticTypeName2': 'SemanticTypeName',\n",
    "                                    'SemanticGroup2': 'SemanticGroup',\n",
    "                                    'SemanticGroupCode2': 'SemanticGroupCode',\n",
    "                                    'BranchPosition2': 'BranchPosition',\n",
    "                                    'CustomTreeNumber2': 'CustomTreeNumber'\n",
    "                                    }, inplace=True)\n",
    "\n",
    "# Save to file so you can open in future sessions, if needed\n",
    "writer = pd.ExcelWriter(localDir + 'logAfterUmlsApi2.xlsx')\n",
    "logAfterUmlsApi2.to_excel(writer,'logAfterUmlsApi2')\n",
    "# df2.to_excel(writer,'Sheet2')\n",
    "writer.save()\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Create files to assign Semantic Types manually\n",
    "# -----------------------------------------------\n",
    "'''\n",
    "If you want to add matches manually using two spreadsheet windows\n",
    "To do in Python - cluster:\n",
    "    - Probable person names\n",
    "    - Probable NLM products, services, web pages\n",
    "    - Probable journal names\n",
    "'''\n",
    "\n",
    "col = ['SemanticGroup', 'SemanticTypeName', 'Definition', 'Examples']\n",
    "SemRef = SemanticNetworkReference[col]\n",
    "\n",
    "# Get class distributions if you want to bolster under-represented sem types\n",
    "\n",
    "currentSemTypeCount = GoldStandard['SemanticTypeName'].value_counts()\n",
    "currentSemTypeCount = pd.DataFrame({'TypeCount':currentSemTypeCount})\n",
    "currentSemTypeCount.sort_values(\"TypeCount\", ascending=True, inplace=True)\n",
    "currentSemTypeCount = currentSemTypeCount.reset_index()\n",
    "currentSemTypeCount = currentSemTypeCount.rename(columns={'index': 'SemanticTypeName'})\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# Visualize results - logAfterUmlsApi2\n",
    "# ------------------------------------\n",
    "    \n",
    "# Pie for percentage of rows assigned; https://pythonspot.com/matplotlib-pie-chart/\n",
    "totCount = len(logAfterUmlsApi2)\n",
    "unassigned = logAfterUmlsApi2['SemanticGroup'].isnull().sum()\n",
    "assigned = totCount - unassigned\n",
    "labels = ['Assigned', 'Unassigned']\n",
    "sizes = [assigned, unassigned]\n",
    "colors = ['lightskyblue', 'lightcoral']\n",
    "explode = (0.1, 0)  # explode 1st slice\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
    "        autopct='%1.f%%', shadow=True, startangle=100)\n",
    "plt.axis('equal')\n",
    "plt.title(\"Status after 'UMLS API 2' processing\")\n",
    "plt.show()\n",
    "\n",
    "# Bar of SemanticGroup categories, horizontal\n",
    "# Source: http://robertmitchellv.com/blog-bar-chart-annotations-pandas-mpl.html\n",
    "ax = logAfterUmlsApi2['SemanticGroup'].value_counts().plot(kind='barh', figsize=(10,6),\n",
    "                                                 color=\"slateblue\", fontsize=10);\n",
    "ax.set_alpha(0.8)\n",
    "ax.set_title(\"Categories assigned after 'UMLS API 2' processing\", fontsize=14)\n",
    "ax.set_xlabel(\"Number of searches\", fontsize=9);\n",
    "# set individual bar lables using above list\n",
    "for i in ax.patches:\n",
    "    # get_width pulls left or right; get_y pushes up or down\n",
    "    ax.text(i.get_width()+.1, i.get_y()+.31, \\\n",
    "            str(round((i.get_width()), 2)), fontsize=9, color='dimgrey')\n",
    "# invert for largest on top \n",
    "ax.invert_yaxis()\n",
    "plt.gcf().subplots_adjust(left=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create new 'uniques' dataframe/file for fuzzy matching\n",
    "# ===========================================================================\n",
    "'''\n",
    "Re-start\n",
    "\n",
    "logAfterUmlsApi1 = pd.read_excel(localDir + 'logAfterUmlsApi1.xlsx')\n",
    "\n",
    "# Set a date range\n",
    "AprMay = logAfterUmlsApi1[(logAfterUmlsApi1['Timestamp'] > '2018-04-01 01:00:00') & (logAfterUmlsApi1['Timestamp'] < '2018-06-01 00:00:00')]\n",
    "\n",
    "logAfterUmlsApi2 = AprMay\n",
    "\n",
    "# Restrict to NLM Home\n",
    "searchfor = ['www.nlm.nih.gov$', 'www.nlm.nih.gov/$']\n",
    "logAfterUmlsApi2 = logAfterUmlsApi2[logAfterUmlsApi2.Referrer.str.contains('|'.join(searchfor))]\n",
    "\n",
    "# Set a date range\n",
    "AprMay = logAfterUmlsApi1[(logAfterUmlsApi1['Timestamp'] > '2018-04-01 01:00:00') & (logAfterUmlsApi1['Timestamp'] < '2018-06-01 00:00:00')]\n",
    "\n",
    "logAfterUmlsApi2 = AprMay\n",
    "'''\n",
    "\n",
    "\n",
    "listOfUniqueUnassignedAfterUmls2 = logAfterUmlsApi2[pd.isnull(logAfterUmlsApi2['preferredTerm'])]\n",
    "listOfUniqueUnassignedAfterUmls2 = listOfUniqueUnassignedAfterUmls2.groupby('adjustedQueryCase').size()\n",
    "listOfUniqueUnassignedAfterUmls2 = pd.DataFrame({'timesSearched':listOfUniqueUnassignedAfterUmls2})\n",
    "listOfUniqueUnassignedAfterUmls2 = listOfUniqueUnassignedAfterUmls2.sort_values(by='timesSearched', ascending=False)\n",
    "listOfUniqueUnassignedAfterUmls2 = listOfUniqueUnassignedAfterUmls2.reset_index()\n",
    "\n",
    "writer = pd.ExcelWriter(localDir + 'unassignedToCheck2.xlsx')\n",
    "listOfUniqueUnassignedAfterUmls2.to_excel(writer,'unassignedToCheck')\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
